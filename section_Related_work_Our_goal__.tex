\section{Related work}
Our goal in this work is to propose a visual quality metric for textured 3D models; since this involves both geometry and 2D image information, we first review related work in image quality assessment and 3D mesh quality assessment. We then focus on the few works that have addressed the visual quality assessment of textured 3D models. For a global view of the topic of visual quality assessment in computer graphics, the reader may refer to \cite{Lavoue2015}.

\subsection{Visual Quality Assessment of 2D Images}
In the field of 2D image processing, the research into objective image quality assessment metrics is highly developed \cite{Wang2006}. Existing algorithms can be classified according to the availability of a reference image: full reference (FR), no-reference (NR) and reduced-reference (RR). The following discussion only focuses on FR methods, where the original \textit{distortion free} image is known as the reference image. Since the pioneer work of \citet{Mannos_1974}, a lot of metrics have been introduced which aim at replacing the classical peak-signal-to-noise ratio (PSNR) which does not correlate with the human vision.
Many techniques have tried to mimic the low-level mechanisms of the human visual system (HVS) such as the \textit{contrast sensitivity function} (CSF), usually modled by a band-pass filter, and the \textit{visual masking} effect which defines the fact that one visual pattern can hide the visibility of another. These bottom-up approaches include the Sarnoff Visual Discrimination model (VDM) \cite{Lubin1993}, the Visible Difference Predictor (VDP) \cite{Daly1993} and the more recent HDR-VDP-2 \cite{Mantiuk2011}, suited for any range of luminance. These computational metrics mostly focus on the visual detectability of near-threshold distortions and are usually less efficient to quantify the visual fidelity (i.e. supra-threshold distortions), except some works like VSNR \cite{Chandler2007} which explicitly incorporates both models. 
In contrast to these computational bottom-up approaches, some authors proposed top-down metrics which do not take into account any comprehensive HVS model but instead operates based on some intuitive hypotheses of what the HVS attempts to achieve when shown a distorted image. The most well-known example is the SSIM Structural SIMilarity index (SSIM)~\cite{Wang_2004} and its derivatives (Multi-Scale SSIM \cite{Wanga} and Information-weighted SSIM \cite{Zhou_Wang_2011}). With a more theoretical definition, the VIF \cite{Sheikh2006} has been developed with the aim to quantify the loss of image information due to the distortion process. Recent surveys and benchmarks \cite{Zhang2012} point out the superiority of these top-down approaches for visual fidelity/quality prediction.

\subsection{Visual Quality Assessment of 3D meshes}
Inspired by image quality metrics, several perceptually-motivated metrics have been designed for 3D meshes. They are all \textit{full-reference} and attemp to predict the visual fidelity of a given 3D mesh (subject to various geometric distortions) with respect to a reference one. The first authors who tried to incorporate some perceptual insights to improve the reliability of geometric distortion measures were Karni and Gotsman \cite{KARNI2000} who proposed combining the Root Mean Square (RMS) distance between corresponding vertices with the RMS distance of their Laplacian coordinates (which reflect a degree of smoothness of the surface). Lavou\'e \cite{Lavoue2011} and Torkhani et al. \cite{Torkhani2012} proposed metrics based on local differences of curvature statistics, while Vasa and Rus \cite{Vasa2012} considered the dihedral angle differences. These metrics consider local variations of attribute values at vertex or edge level, which are then pooled into a global score. In contrast, Corsini et al. \cite{Corsini2007} and Wang et al. \cite{Wang2012} compute global roughness values per model and then derive a simple global roughness difference. Similarly to bottom-up image quality metrics, some of these latter algorithms \cite{Torkhani2012,Wang2012,Vasa2012} integrate perceptually motivated mechanisms such as visual masking. A recent surveys \cite{Corsini2013} details these works and compares their performance regarding their correlation with mean opinion scores derived from subjective rating experiments; this study shows that MSDM2 \cite{Lavoue2011}, FMPD \cite{Wang2012} and DAME \cite{Vasa2012} are excellent predictors of visual quality. Besides these works on global visual fidelity assessment (suited for supra-threshold distortions), several relevant works were introduced very recently: \citet{Nader2016} introduced a bottom-up visibility threshold predictor for 3D meshes (assuming a flat-shaded rendering), and \citet{Guo2015} studied the \textit{local} visibility of geometric artifacts and showed that curvature maybe a good predictor of local distortions. Finally a comprehensive study was introduced by \citet{GuillaumeLavoue2016} to investigate the use of image metrics (computed on rendered images) for assessing the visual quality of 3D model (without texture). It shows that some of them (MS-SSIM, in particular) may provide excellent performance. One of the contribution of the present work, is to validate if this conclusion still hold for textured meshes.

\subsection{Visual Quality Assessment of textured 3D models}
Only few works can be found in the literature dealing with the quality assessment of textured 3D models. Existing works \cite{Tian_2004,Yang2004,Pan2005} are mostly dedicated to choosing the appropriate mesh and texture levels of detail (LoD) for optimizing the progressive transmission of textured meshes. \cite{Pan2005} introduced a metric directly based on mesh and texture resolutions, fitted on subjective data. Whereas this metric is efficient for the purpose of optimizing transmission \cite{Cheng2007}, it cannot generalize to other types of distortions. \citet{Tian_2004,Tian2008} proposed the Fast quality measure (FQM) as a weighted combination of two simple error measures: the mean squared error (MSE) over mesh vertices (approximated in the 2D domain for efficiency issue) and the MSE over texture pixels. The weighting coefficient between those two measures is computed by studying their influence on the screen-space error (considered as a meaningful prediction of the subjective perceptual quality). The FQM metric wasn't subject to any perceptual validation by a subjective experiment ; in the experimental section, we compare our results against it. \citet{Yang2004} and \citet{Griffin2015} also considered image metrics computed on rendered images as \textit{oracles} of the subjective quality. \citet{Yang2004} used image MSE and Mannos model \cite{Mannos_1974} for optimizing textured mesh transmission and \citet{Griffin2015} considered SSIM \cite{Wang_2004} to evaluate the masking effect between texture and normal maps (in the context of compression artifacts). Like stated in the above subsection, our subjective data will allow to challenge this hypothesis of performance of screen-space image metric for textured meshes. \\
Finally, two related works are those from \citet{FERWERDA:1997} and \citet{Qu2008}. The authors aim at evaluating how the texture is able to mask geometric distortions (from simplification and remeshing), whereas our objective is to evaluate the effect of texture distortions (possibly combined with geometric distortions) on the final appearance.
%Herzog et al.,Guthe
%un point important pour nus, pas besoin de faire de rendu

%Due to the scarcity of textured 3D mesh visual quality metrics, most quality assessments focus on the rendering videos or screen displays, containing textured 3D contents. In fact, they were extended from images visual quality assessments \cite{Seshadrinathan_2010}. For instant, some video assessments \cite{Zhou_Wang} apply SSIM or MS-SSIM index to video frame-by-frame on the luminance component of the video, and compute the average SSIM or MS-SSIM index over all the frames as the visual quality score. Then, Pinson et al. \cite{Pinson_2004} proposed a more perceptual correlated but complex assessment, video quality metric (VQM), which calculates the perceptual changes in spatial, temporal and chrominance properties from spatial-temporal sub-regions of video streams, and then pools them into a single quality score. 