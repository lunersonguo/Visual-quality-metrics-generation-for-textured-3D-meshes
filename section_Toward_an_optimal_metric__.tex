\section{Toward an optimal metric for textured mesh quality assessment}
\subsection{Novel geometric metric}
Through observations of geometric distortions (smoothing, quantization and simplification), we found that the order of the distortions with the highest strengths from three types shows a regularity, which is that the model ranked with the worst visual quality always comes from either models processed by the strongest quantization method or those processed by the strongest simplification method, and the models with strongest smoothing distortions never appear in the end of ranks. For instance, among 20 distorted Hulk models, the distortion with the worst visual quality is the quantization with 7 bits(vote score: 0.91), while the distortion of 4 iterations smoothing has a fairly high subjective vote score(7.45) (see Figure 8).  Similar cases happen in other different models. This regularity implies some mechanisms of visual systems on sensing the variances of geometric surfaces, which means HVS is more sensitive to the variances from local areas (distortion caused by simplification or quantization) rather than to the global slight variances (distortion caused by smoothing). Meanwhile, previous study \cite{Guo_2015} shows that HVS is highly adapted for extracting curvature as surface structural information.\\
Based on the observations and previous studies, we propose a novel local distortion measurement by computing the amount of variances of curvature in local neighborhoods between two meshes (distorted mesh: $M_d$ and reference mesh $M_r$).  In details, at first we use fast matching method to establish a correspondence between $M_d$ and $M_r$, obtaining all the curvatures {$C_i$} from $M_d$ and their corresponding curvature {$\hat{C_i}$} on the surface $M_r$. For each vertex {$v_i$} from $M_d$, we obtain the standard deviation $\delta_i$ of local curvature differences in a small surface region $h$, in which there $K$ vertices and points are enclosed . For $M_d$ and $M_r$, $\delta_i$ is computed as follows:
\begin{equation}
\delta_i =\sqrt{\frac{1}{K}\sum_{j=1}^{K}{((\hat{C_j}-C_j)-E(\hat{C_j}-C_j))^2}}
\end{equation}
  
 $E(\hat{C_j}-C_j)$ is the mean value of curvatures differences in the region $h$.Refer to the work in \cite{Lavou__2011}, for each vertex we also measured the differences in different scales($h_i$) to capture the perceptually meaningful scales, and improve the efficiency and robustness of the metric. We took three scales $h_i\in\{2\epsilon, 3\epsilon, 4\epsilon\}$, where $\epsilon = 2.5\%$ of the max length of the bounding box of the model.  Then Multi-scale local distortion measure $\delta_M(v)$ was computed as: $\delta_M(v) = \frac{1}{n}\sum_{i=1}^n{\delta}_v^{h_i}$. $n$ is the number of scales (3 in our study). Finally, through extensive trials, we utilized root mean square (RMS) to pool all the local measurements together and obtained our global SDCD (Standard Deviation of Curvature Difference) value: $G\Delta M_d=(\frac{1}{|M_d|}\sum_{v\in M_d }{ \delta_M(v)^2})^\frac{1}{2}$. This metric emphasizes on variances of local roughness deviations, whereas the overall changes of a mesh (e.g., global shrink of a model) are not focally considered. The performance of this metric will be evaluated and validated in the following subsection.\\
\subsection{Mesh and  Image metric evaluation}
Since all the distorted models have mono-type of attacks(either geometric or textural attacks),the subjective vote scores can be also used as ground truths to evaluate the existing geometric and imaging measurements respectively, and consequently determine the most appropriate measurements for the optimal combinations. For all the distorted models, we firstly split them into two groups: one group only with geometric distortions and the other only with textural distortions. Then we select several commonly used perceptually geometric and image metrics to measure the correspondent models, which means the geometric metrics measure the models in geometric distortions group, while the image metrics measure the models in textural distortions group. For the measurements of geometric distortions, we choose MSE metric, MSDM2 \cite{Lavou__2011}, which is one of the best performed perceptually-motivated metrics, and our newly proposed metric: SDCD, whilst we select MSE, SSIM \cite{Wang_2004} and MS-SSIM \cite{Wang} metrics for the measurements of textural distortions.  Since each model has a vote score from subjects, in each group, we consider all the vote score as ground truth to evaluate the performances of metrics.  Table 3 shows the Spearman and Pearson correlations between the geometric metrics and the vote scores in geometric distortions group, and Table 4 presents the two correlations between the image metrics and the vote scores in textural distortions group.\\
\begin{table}[]
\centering
\caption{Pearson($r_p$) and Spearman rank( $r_s$) correlation values(\%) and RMSE values between geometric metrics and ground truths(vote scores) in geometric distortions group}
\label{my-label}
\begin{tabular}{lllllllllll}
                          &  \textbf{Squirrel} & \textbf{} & \textbf{Hulk} & \textbf{} & \textbf{Easter Island Statue} & \textbf{} & \textbf{Sport Car} & \textbf{} & \textbf{All Models} &      \\
                          & $r_p$        & $r_s$        & $r_p$            & $r_s$        & $r_p$               & $r_s$        & $r_p$                & $r_s$       & $r_p$                  & $r_s$   \\
 \textbf{MSE}           & 71.6 & \textbf{90.1} & 37.1 & 60.8 & 7.1  &  0        & 64.7 & 28.0 & 26.0          & 29.8         \\
\textbf{MSDM2} \cite{Lavou__2011}            & \textbf{77.5} & 75.5 & \textbf{87.9} & \textbf{84.6} & 15.1                 & 15.4          & \textbf{53.2} & \textbf{52.4} & 47.2          & 38.9          \\
\textbf{SDCD}(Our Metric) & 59.4          & 55.2          & 83.8          & 79.7          & \textbf{64.3}        & \textbf{63.6} & 39.3          & 41.3          & \textbf{58.7} & \textbf{58.5}
\end{tabular}
\end{table}

\begin{table}[]
\centering
\caption{Pearson($r_p$) and Spearman rank( $r_s$) correlation values(\%) and RMSE values between image metrics and ground truths(vote scores) in textural distortions group}
\label{my-label}
\begin{tabular}{lllllllllll}
                & \textbf{Squirrel} & \textbf{} & \textbf{Hulk} & \textbf{} & \textbf{Easter Island Statue} & \textbf{} & \textbf{Sport Car} & \textbf{} & \textbf{All Models} &      \\
                & $r_p$        & $r_s$        & $r_p$            & $r_s$        & $r_p$               & $r_s$        & $r_p$                & $r_s$       & $r_p$                  & $r_s$   \\
\textbf{MSE}    & 72.9              & 85.7          & 20.8  & 27.5          & 38.9                          & 50.0          & \textbf{92.5}      & \textbf{99.9} & 36.8                & 28.9          \\
\textbf{SSIM} \cite{Wang_2004}  & 27.0              & 16.7          & 59.6          & 76.6          & 70.2                          & 69.0          & 90.2               & 92.9          & 23.4                & 20.2          \\
\textbf{MS-SSIM} \cite{Wang} & \textbf{82.7}     & \textbf{97.6} & \textbf{67.3}          & \textbf{81.4} & \textbf{69.7}                 & \textbf{76.2} & 85.6               & 85.7          & \textbf{47.8}       & \textbf{59.1}
\end{tabular}
\end{table}
Table 3 shows that, on measuring some models, MSDM2 performs better than SDCD with respect to the ground truths, however, on the Easter Island Statues model SDCD shows a significant improvement and outperforms when considering all the models together. In consideration of SDCD’s unsatisfying performance on some models, both MSDM2 and SDCD metrics are opted for the optimal combinations with textural metrics. Table 4 shows that MS-SSIM metric generally outperforms among 3 metrics, even though the correlation values of Sport Car are somewhat lower than MSE.  Hence, we choose MS-SSIM as our textural metric for the optimal combinations.\\
\subsection{Toward an optimal combination}
Since we have determined the geometric measure $Q_G$ (MSDM2 or SDCD) and the textural measure $Q_T$ for the combination, finding an optimal way to combine $Q_G$ and $Q_T$ is critical to generate a highly performed visual quality metric for textured 3D meshes. Refer to the work \cite{Tian_2004} , we set the combination $CM$ in the following fashion as well:
\begin{equation}
CM = \alpha Q_G +(1-\alpha) Q_T
\end{equation}
where $\alpha$ is a weight for combining two measures. Then we utilized machine learning mechanism to determine $\alpha$. In details,  all the 60 distorted models of 3 kinds(e.g., Squirrel, Easter Island statue and Sport Car) are considered as a training dataset. All their meshes and textures are measured by $Q_G$ and $Q_T$ respectively. Then, From $\alpha =0$ to$1$, we launch a massive searching (step: 0.001) to find an optimal weight $\alpha(k)$, with which the correspondent $CM(k)$ values have the highest Spearman correlation with the ground truths(vote scores). Later, we apply $CM(k)$ to measure the testing dataset(e.g., the rest 20 Hulk models). Through computing the Spearman correlation between the measures of $CM(k)$ values from 20 Hulk models and the ground truths, we can evaluate and validate $\alpha(k)$. We present the all validations and evaluations in following parts.\\
\subsubsection{Evaluations of the combinations}
As we presented before, we used a cross-validation like protocol to optimize the combinations of geometric and textural measurements: $CM_1$(the optimal combination of MSDM2 and MS-SSIM) and $CM_2$(the optimal combination of SDCD and MS-SSIM). To evaluate the optimal combinations, we also introduced FQM and some well-known Video Quality Measurements: VQM, VQM-PSNR and VQM-MSSSIM, the last two of which are embedded with image quality metrics, for the comparison. Since previously FQM was not subjected to perceptual validations by any subjective experiment, in our study we used our protocol to determine the optimal weight for $Q_G$ and $Q_T$. Then, we computed the Spearman rank and Pearson correlations between the measured values and ground truths, and also calculated RMSE values of residuals. All results are shown in the Table 5. Meanwhile, Figure 9 shows the scatter plot of vote score versus metric predictions for each model, the best performed video quality metric(VQM-MSSSIM) according to Table 5, FQM, $CM_1$ and $CM_2$ are chosen for evaluating the effectiveness of all the metrics.\\
\begin{table}[]
\centering
\caption{Pearson($r_p$) and Spearman rank( $r_s$) correlation values(\%) and RMSE values between different quality metrics for textured 3D models and ground truths(vote scores). Rendering proposed by \cite{Rogowitz_2001}}
\label{my-label}
\begin{tabular}{llllllllllllllll}
                    &      & \textbf{Squirrel} & \textbf{} & \textbf{} & \textbf{Hulk} & \textbf{} & \textbf{} & \textbf{Easter Island Statue} & \textbf{} & \textbf{} & \textbf{Sport Car} & \textbf{} & \textbf{} & \textbf{Dwarf} &      \\
                     & $r_p$                & $r_s$           & RMSE & $r_p$                & $r_s$            & RMSE & $r_p$                & $r_s$            & RMSE &$r_p$                & $r_s$            & RMSE      & $r_p$                & $r_s$             & RMSE \\
\textbf{VQM}        & 12.0          & 9.5               & 4.76          & 18.3          & 35.5          & 4.99          & 26.5          & 30.0                          & 5.04          & 62.2          & 67.2               & 3.94          & 50.1          & 31.7           & 7.81          \\
\textbf{VQM-PSNR}   & 21.7          & 26.0              & 4.68          & 33.1          & 35.8          & 4.81          & 21.2          & 26.3                          & 5.12          & 67.0          & 70.0               & 3.71          & 58.3          & 33.0           & 7.32          \\
\textbf{VQM-MSSSIM} & 24.5          & 38.9              & 4.64          & 16.8          & 41.3          & 5.00          & 25.3          & 42.1                          & 5.05          & \textbf{66.8} & \textbf{72.3}      & \textbf{3.64} & 66.1          & 66.6           & 6.79          \\
\textbf{FQM}        & 51.2          & 68.9              & 3.97          & 28.6          & 43.8          & 4.85          & 25.3          & 21.5                          & 5.07          & 68.2          & 45.0               & 3.69          & 64.0          & 66.4           & 6.9           \\
$CM_1$                 & \textbf{82.0} & \textbf{81.7}     & \textbf{2.35} & \textbf{69.2} & \textbf{87.7} & \textbf{2.84} & 43.4          & 47.7                          & 4.71          & 65.6          & 53.8               & 3.81          & 75.6          & 78.7           & 5.82          \\
$CM_2$                 & 78.1          & 71.3              & 2.80          & 63.6          & 81.5          & 3.29          & \textbf{58.0} & \textbf{67.7}                 & \textbf{4.14} & 64.1          & 49.2               & 3.88          & \textbf{82.2} & \textbf{86.6}  & \textbf{4.93}
\end{tabular}
\end{table}
Trough Table 5 and Figure 9, we can see that the optimal combinations ($CM_1$ and $CM_2$) outperform among all the existing visual quality metrics for the Squirrel, Hulk and Easter Island statue. However, for the Sport Car, it is quite a reverse that video quality metrics (VQM, VQM-PSNR and VQM-MSSSIM) produces the better results than combined metrics (FQM, $CM_1$ and $CM_2$) with respect to the ground truths. The major reason of this phenomenon is that, unlike other models, the Sport Car model consists of many independent pieces of 3D meshes (e.g., wheels, exhaust pipes, etc.) (see Figure 10). When applying certain strengths of geometric attacks, such like Laplacien smoothing and simplification, each piece of mesh is processed by the distortion algorithms, and thus altered its appearance. Consequently, some small parts of the Sport Car are significantly altered, and thus more obvious to be detected by the observers. Figure 10 shows an example of distorted model with Laplacien smoothing, and the exhaust system is greatly distorted. Since the observer’s task is to select the model “closer” to the reference, these kind of partial differences become biases of the criteria on subjective selections, and thereby influence the ground truths. Meanwhile, the partial differences can be more effectively measured by screen-space error measurements, such as VQM, VQM-PSNR, and VQM-MSSSIM, while the geometric measure $Q_G$ of a combination pools all mesh surface differences into a single value, which may not primarily represent these partial differences.\\
\subsubsection{Validation using mixed attacks}
Since our optimal combinations are determined by the ground truths from the models with mono-type of attacks (either geometric or textural attacks), whether they are suitable for the models with mixed types of attacks (both geometric and textural attacks) needs validating. For this purpose, we used our 36 Dwarf models with mixed distortions to evaluate all the metrics and validate the effectiveness of our optimal combinations. Meanwhile, to challenge our proposed experimental procedure, we introduced a well-known sorting algorithm, AVL tree, for this mixed distorting setting, and obtained a global ranking of 36 stimuli, which needs about 140 rounds of comparisons for each subject in our study. Similarly, we trained on the 80 models (Squirrel, Hulk, Easter Island Statue and Sport Car) to determine the optimal weights for our combinations, and tested on 36 Dwarf models for the validation. All the other metrics were also used as comparisons for the evaluations.The performance results for the Dwarf are presented in the last column of Table 5, and the scatter plot, Figure 11.
From the Table 5 and Figure 10, we can see that our combinations outperform among all the metrics, which also validates that the optimal combinations determined by the mono-type of attacks can be suitable for  the predictions of mixed types of attacks as well.\\

In order to analyze the influence of rendering on the optimal combinations, we used our cross-validation like protocol to optimize the combinations of FQM, $CM_1$ and $CM_2$ based the ground truths from videos rendering settings only with reflectance. Similarly, we also introduced VQM, VQM-PSNR and VQM-MSSSIM for the measures of the videos with this rendering. Then, we computed the Spearman rank and Pearson correlations between the measured values and ground truths, and also calculated RMSE values of residuals. All results are shown in the Table 7.\\
\begin{table}[]
\centering
\caption{Pearson($r_p$) and Spearman rank( $r_s$) correlation values(\%) and RMSE values between different quality metrics for textured 3D models and ground truths(vote scores) Rendering only with reflectance.}
\label{my-label}
\begin{tabular}{llllllllllllllll}
                    & \textbf{}     & \textbf{Squirrel} & \textbf{}     & \textbf{}     & \textbf{Hulk} & \textbf{}     & \textbf{Easter Island Statue}     & \textbf{} & \textbf{}     & \textbf{}     & \textbf{Sport Car} & \textbf{}     & \textbf{}     & \textbf{Dwarf} &               \\
                     & $r_p$                & $r_s$           & RMSE & $r_p$                & $r_s$            & RMSE & $r_p$                & $r_s$            & RMSE &$r_p$                & $r_s$            & RMSE      & $r_p$                & $r_s$             & RMSE \\
\textbf{VQM}        & -21.1         & -25.9             & 4.76          & 21.5          & 24.5          & 4.72          & 34.3          & 24.1                          & 4.73          & 57.5          & 42.1               & 4.18          & 34.7          & 31.7           & 8.23          \\
\textbf{VQM-PSNR}   & -12.7         & -4.2              & 4.83          & 34.7          & 39.4          & 4.53          & 13.2          & 21.8                          & 4.97          & 50.9          & 55.7               & 4.34          & 40.1          & 33.0           & 8.23          \\
\textbf{VQM-MSSSIM} & -5.3          & 13.7              & 4.86          & 27.6          & 61.2          & 4.64          & 34.3          & 37.3                          & 4.72          & 66.8          & 68.7               & 3.77          & 68.3          & 66.6           & 6.42          \\
\textbf{FQM}        & 38.3          & 63.0              & 4.44          & 34.5          & 38.5          & 4.52          & 49.0          & 41.7                          & 4.38          & 72.9          & 56.2               & 3.29          & 76.2          & 78.5           & 5.73          \\
$CM_1$                 & \textbf{80.4} & \textbf{87.2}     & \textbf{2.46} & \textbf{68.7} & \textbf{85.8} & \textbf{2.87} & 54.9          & 46.3                          & 4.22          & \textbf{77.3} & \textbf{89.5}      & \textbf{3.08} & 85.4          & 85.8           & 4.47          \\
$CM_2$                 & 79.0          & 80.5              & 2.59          & 64.2          & 78.3          & 3.24          & \textbf{63.0} & \textbf{66.2}                 & \textbf{3.87} & 75.3          & 86.9               & 3.23          & \textbf{85.9} & \textbf{86.9}  & \textbf{4.42}
\end{tabular}
\end{table}

Through Table 7, we can see that our combinations outperform over all the models under the rendering only with reflectance. Under this rendering, user mainly focused on the textures, since the geometries were not effectively rendered and shown. Thus, the HVS perception cannot capture geometric variances under this rendering, while the video quality metrics can measure the subtle geometric variations. \\

\begin{table}[]
\centering
\caption{My caption}
\label{my-label}
\begin{tabular}{lllllll}
             &               & TrainOldTestNew &               &               & TrainNewTestOld &               \\
             & $r_p$                & $r_s$             & RMSE          & $r_p$                & $r_s$               & RMSE          \\
\textbf{FQM} & 33.2          & 32.5           & 4.74          & 37.1         & 52.4            & 4.53          \\
\textbf{$CM_1$} & 45.0         & 56.9            & 4.40          & 53.0          & 66.2            & 3.89          \\
\textbf{$CM_2$} & \textbf{45.2} & \textbf{64.2}   & \textbf{4.38} & \textbf{52.4} & \textbf{69.4}   & \textbf{3.92}
\end{tabular}
\end{table}